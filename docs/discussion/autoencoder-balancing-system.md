# Defense Allies ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± ì‹œìŠ¤í…œ

## ğŸ“‹ ë¬¸ì„œ ì •ë³´
- **ì‘ì„±ì¼**: 2024ë…„
- **ë²„ì „**: v1.0
- **ëª©ì **: ì˜¤í† ì¸ì½”ë” êµ¬ì¡° ê¸°ë°˜ ê²Œì„ ë°¸ëŸ°ì‹± ì‹œìŠ¤í…œ ì„¤ê³„
- **í˜ì‹ **: ì„¸ê³„ ìµœì´ˆ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì‹¤ì‹œê°„ ê²Œì„ ë°¸ëŸ°ì‹±

## ğŸ§  ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± ê°œë…

### í•µì‹¬ ì•„ì´ë””ì–´
```yaml
ê²Œì„ ë°¸ëŸ°ì‹± = ì˜¤í† ì¸ì½”ë” êµ¬ì¡°

Input Layer (ì¸ì½”ë”):
  - ê²Œì„ ë””ìì´ë„ˆê°€ ì›í•˜ëŠ” ê° íƒ€ì›Œë“¤ì˜ ì´ìƒì  ë§¤íŠ¸ë¦­ìŠ¤
  - ì¢…ì¡±ë³„ íŠ¹ì„± ë§¤íŠ¸ë¦­ìŠ¤
  - í™˜ê²½ ë³€ìˆ˜ ë§¤íŠ¸ë¦­ìŠ¤
  - í”Œë ˆì´ì–´ ì„ í˜¸ë„ ë§¤íŠ¸ë¦­ìŠ¤

Bottleneck (ì ì¬ ê³µê°„):
  - ê²Œì„ ë‚œì´ë„ ë²¡í„° (1ì°¨ì›)
  - ì „ì²´ ê²Œì„ ë°¸ëŸ°ìŠ¤ ìŠ¤ì¹¼ë¼ ê°’
  - í˜‘ë ¥ ê°•ë„ ê³„ìˆ˜
  - í™˜ê²½ ì˜í–¥ë„

Output Layer (ë””ì½”ë”):
  - ì‹¤ì œ ê²Œì„ì—ì„œ ì ìš©ë˜ëŠ” ìµœì¢… íƒ€ì›Œ ë§¤íŠ¸ë¦­ìŠ¤
  - ë™ì ìœ¼ë¡œ ì¡°ì •ëœ ë°¸ëŸ°ìŠ¤ ë§¤íŠ¸ë¦­ìŠ¤
  - ì‹¤ì‹œê°„ í™˜ê²½ ë³´ì • ë§¤íŠ¸ë¦­ìŠ¤
```

### ìˆ˜í•™ì  êµ¬ì¡°
```python
# ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± í•¨ìˆ˜
def autoencoder_balancing(designer_matrices, game_state):
    # ì¸ì½”ë”: ë³µì¡í•œ ê²Œì„ ìƒíƒœë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•
    latent_vector = encoder(designer_matrices, game_state)

    # ë³´í‹€ë„¥: í•µì‹¬ ê²Œì„ íŒŒë¼ë¯¸í„°
    difficulty = latent_vector[0]      # ë‚œì´ë„ (-1 ~ +1)
    balance_target = latent_vector[1]  # ë°¸ëŸ°ìŠ¤ ëª©í‘œ (0 ~ 1)
    cooperation_weight = latent_vector[2]  # í˜‘ë ¥ ê°€ì¤‘ì¹˜ (0 ~ 1)

    # ë””ì½”ë”: ì €ì°¨ì›ì—ì„œ ì‹¤ì œ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³µì›
    final_matrices = decoder(latent_vector, designer_matrices)

    return final_matrices
```

## ğŸ”§ ì¸ì½”ë” ì„¤ê³„ (Input â†’ Bottleneck)

### ì…ë ¥ ë°ì´í„° êµ¬ì¡°
```python
class GameBalanceInput:
    """ì˜¤í† ì¸ì½”ë” ì…ë ¥ ë°ì´í„°"""

    def __init__(self):
        # 1. ë””ìì´ë„ˆ ì˜ë„ ë§¤íŠ¸ë¦­ìŠ¤ (162ê°œ íƒ€ì›Œ)
        self.designer_matrices = np.zeros((162, 2, 2))  # 18ì¢…ì¡± Ã— 9íƒ€ì›Œ Ã— 2Ã—2

        # 2. í˜„ì¬ ê²Œì„ ìƒíƒœ
        self.current_game_state = {
            'player_count': 4,
            'game_progress': 0.5,  # 0~1
            'average_skill': 0.7,  # 0~1
            'cooperation_level': 0.6  # 0~1
        }

        # 3. í™˜ê²½ ì»¨í…ìŠ¤íŠ¸
        self.environment_context = {
            'time': 'day',
            'weather': 'clear',
            'terrain': 'forest',
            'active_events': ['meteor_shower']
        }

        # 4. í”Œë ˆì´ì–´ í”¼ë“œë°±
        self.player_feedback = {
            'difficulty_rating': 0.8,  # ë„ˆë¬´ ì–´ë ¤ì›€ = 1.0
            'balance_satisfaction': 0.6,  # ë¶ˆë§Œì¡± = 0.0
            'cooperation_enjoyment': 0.9  # ì¬ë¯¸ì—†ìŒ = 0.0
        }

def encode_to_latent_space(input_data: GameBalanceInput) -> np.ndarray:
    """ë³µì¡í•œ ê²Œì„ ìƒíƒœë¥¼ 3ì°¨ì› ì ì¬ ê³µê°„ìœ¼ë¡œ ì••ì¶•"""

    # 1. ë””ìì´ë„ˆ ë§¤íŠ¸ë¦­ìŠ¤ ë¶„ì„
    designer_complexity = analyze_designer_intent(input_data.designer_matrices)

    # 2. ê²Œì„ ìƒíƒœ ë¶„ì„
    game_dynamics = analyze_game_dynamics(input_data.current_game_state)

    # 3. í™˜ê²½ ì˜í–¥ë„ ë¶„ì„
    environment_impact = analyze_environment_impact(input_data.environment_context)

    # 4. í”Œë ˆì´ì–´ ë§Œì¡±ë„ ë¶„ì„
    player_satisfaction = analyze_player_feedback(input_data.player_feedback)

    # 5. ì ì¬ ë²¡í„° ê³„ì‚°
    difficulty = calculate_target_difficulty(
        game_dynamics, player_satisfaction, designer_complexity
    )

    balance_target = calculate_balance_target(
        designer_complexity, environment_impact, player_satisfaction
    )

    cooperation_weight = calculate_cooperation_weight(
        input_data.current_game_state['cooperation_level'],
        input_data.player_feedback['cooperation_enjoyment']
    )

    return np.array([difficulty, balance_target, cooperation_weight])

def analyze_designer_intent(designer_matrices: np.ndarray) -> Dict:
    """ë””ìì´ë„ˆ ì˜ë„ ë¶„ì„"""

    # ì „ì²´ ë§¤íŠ¸ë¦­ìŠ¤ì˜ í†µê³„ì  íŠ¹ì„±
    all_matrices_flat = designer_matrices.reshape(-1, 4)  # 162Ã—4

    complexity_score = np.var(all_matrices_flat, axis=0).mean()  # ë¶„ì‚° ê¸°ë°˜ ë³µì¡ë„
    power_distribution = np.std([np.linalg.norm(m, 'fro') for m in designer_matrices])
    specialization_degree = calculate_specialization_variance(designer_matrices)

    return {
        'complexity': complexity_score,
        'power_variance': power_distribution,
        'specialization': specialization_degree
    }

def calculate_target_difficulty(game_dynamics: Dict,
                              player_satisfaction: Dict,
                              designer_complexity: Dict) -> float:
    """ëª©í‘œ ë‚œì´ë„ ê³„ì‚° (-1: ì‰½ê²Œ, +1: ì–´ë µê²Œ)"""

    # í”Œë ˆì´ì–´ê°€ ë„ˆë¬´ ì‰½ë‹¤ê³  ëŠë¼ë©´ ë‚œì´ë„ ì¦ê°€
    if player_satisfaction['difficulty_rating'] < 0.3:
        difficulty_adjustment = +0.5
    elif player_satisfaction['difficulty_rating'] > 0.8:
        difficulty_adjustment = -0.5
    else:
        difficulty_adjustment = 0.0

    # ê²Œì„ ì§„í–‰ë„ì— ë”°ë¥¸ ì¡°ì •
    progress_factor = (game_dynamics['game_progress'] - 0.5) * 0.3

    # ë””ìì´ë„ˆ ë³µì¡ë„ ë°˜ì˜
    complexity_factor = (designer_complexity['complexity'] - 1.0) * 0.2

    target_difficulty = difficulty_adjustment + progress_factor + complexity_factor

    return np.clip(target_difficulty, -1.0, 1.0)
```

## ğŸ¯ ë³´í‹€ë„¥ ì„¤ê³„ (Latent Space)

### 3ì°¨ì› ì ì¬ ê³µê°„
```python
class LatentGameState:
    """ê²Œì„ì˜ í•µì‹¬ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì ì¬ ê³µê°„"""

    def __init__(self, latent_vector: np.ndarray):
        self.difficulty = latent_vector[0]        # [-1, +1] ë‚œì´ë„ ì¡°ì •
        self.balance_target = latent_vector[1]    # [0, 1] ë°¸ëŸ°ìŠ¤ ëª©í‘œ
        self.cooperation_weight = latent_vector[2] # [0, 1] í˜‘ë ¥ ê°€ì¤‘ì¹˜

    def interpret_state(self) -> Dict[str, str]:
        """ì ì¬ ìƒíƒœ í•´ì„"""

        # ë‚œì´ë„ í•´ì„
        if self.difficulty < -0.5:
            difficulty_desc = "ë§¤ìš° ì‰¬ì›€"
        elif self.difficulty < 0:
            difficulty_desc = "ì‰¬ì›€"
        elif self.difficulty < 0.5:
            difficulty_desc = "ì–´ë ¤ì›€"
        else:
            difficulty_desc = "ë§¤ìš° ì–´ë ¤ì›€"

        # ë°¸ëŸ°ìŠ¤ ëª©í‘œ í•´ì„
        if self.balance_target < 0.3:
            balance_desc = "ë¶ˆê· í˜• í—ˆìš©"
        elif self.balance_target < 0.7:
            balance_desc = "ì ë‹¹í•œ ë°¸ëŸ°ìŠ¤"
        else:
            balance_desc = "ì™„ë²½í•œ ë°¸ëŸ°ìŠ¤"

        # í˜‘ë ¥ ê°€ì¤‘ì¹˜ í•´ì„
        if self.cooperation_weight < 0.3:
            coop_desc = "ê°œì¸ í”Œë ˆì´ ì¤‘ì‹¬"
        elif self.cooperation_weight < 0.7:
            coop_desc = "ê· í˜•ì¡íŒ í˜‘ë ¥"
        else:
            coop_desc = "í˜‘ë ¥ í•„ìˆ˜"

        return {
            'difficulty': difficulty_desc,
            'balance': balance_desc,
            'cooperation': coop_desc
        }

    def generate_adjustment_strategy(self) -> Dict[str, float]:
        """ì¡°ì • ì „ëµ ìƒì„±"""

        return {
            'power_scaling': 1.0 + self.difficulty * 0.3,  # ë‚œì´ë„ì— ë”°ë¥¸ íŒŒì›Œ ìŠ¤ì¼€ì¼ë§
            'variance_tolerance': self.balance_target,       # ë°¸ëŸ°ìŠ¤ í—ˆìš© ì˜¤ì°¨
            'synergy_multiplier': 1.0 + self.cooperation_weight * 0.5,  # ì‹œë„ˆì§€ ê°•í™”
            'individual_penalty': self.cooperation_weight * 0.2  # ê°œì¸ í”Œë ˆì´ í˜ë„í‹°
        }

def visualize_latent_space(latent_vector: np.ndarray) -> str:
    """ì ì¬ ê³µê°„ ì‹œê°í™”"""

    state = LatentGameState(latent_vector)
    interpretation = state.interpret_state()
    strategy = state.generate_adjustment_strategy()

    visualization = f"""
    ğŸ® ê²Œì„ ìƒíƒœ ë¶„ì„:

    ğŸ“Š ì ì¬ ë²¡í„°: [{latent_vector[0]:.2f}, {latent_vector[1]:.2f}, {latent_vector[2]:.2f}]

    ğŸ¯ í•´ì„:
    - ë‚œì´ë„: {interpretation['difficulty']}
    - ë°¸ëŸ°ìŠ¤: {interpretation['balance']}
    - í˜‘ë ¥ë„: {interpretation['cooperation']}

    âš™ï¸ ì¡°ì • ì „ëµ:
    - íŒŒì›Œ ìŠ¤ì¼€ì¼ë§: {strategy['power_scaling']:.2f}x
    - ë°¸ëŸ°ìŠ¤ í—ˆìš©ë„: {strategy['variance_tolerance']:.2f}
    - ì‹œë„ˆì§€ ë°°ìœ¨: {strategy['synergy_multiplier']:.2f}x
    - ê°œì¸ í”Œë ˆì´ í˜ë„í‹°: {strategy['individual_penalty']:.2f}
    """

    return visualization
```

## ğŸ”„ ë””ì½”ë” ì„¤ê³„ (Bottleneck â†’ Output)

### ìµœì¢… ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
```python
def decode_to_final_matrices(latent_vector: np.ndarray,
                           designer_matrices: np.ndarray) -> np.ndarray:
    """ì ì¬ ê³µê°„ì—ì„œ ìµœì¢… ê²Œì„ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë””ì½”ë”©"""

    state = LatentGameState(latent_vector)
    strategy = state.generate_adjustment_strategy()

    final_matrices = np.zeros_like(designer_matrices)

    for i, designer_matrix in enumerate(designer_matrices):
        # 1. ê¸°ë³¸ íŒŒì›Œ ìŠ¤ì¼€ì¼ë§
        scaled_matrix = designer_matrix * strategy['power_scaling']

        # 2. ë°¸ëŸ°ìŠ¤ ì¡°ì •
        balanced_matrix = apply_balance_adjustment(
            scaled_matrix, strategy['variance_tolerance']
        )

        # 3. í˜‘ë ¥ ê°€ì¤‘ì¹˜ ì ìš©
        cooperation_matrix = apply_cooperation_weighting(
            balanced_matrix, strategy['synergy_multiplier'], strategy['individual_penalty']
        )

        # 4. ì œì•½ ì¡°ê±´ ì ìš©
        final_matrix = apply_constraints(cooperation_matrix)

        final_matrices[i] = final_matrix

    return final_matrices

def apply_balance_adjustment(matrix: np.ndarray, tolerance: float) -> np.ndarray:
    """ë°¸ëŸ°ìŠ¤ ì¡°ì • ì ìš©"""

    # í˜„ì¬ ë§¤íŠ¸ë¦­ìŠ¤ì˜ ë¶ˆê· í˜• ì¸¡ì •
    current_variance = np.var(matrix)
    target_variance = tolerance * 0.1  # í—ˆìš© ë¶„ì‚°

    if current_variance > target_variance:
        # ë¶„ì‚°ì´ ë„ˆë¬´ í¬ë©´ í‰ê· ìœ¼ë¡œ ìˆ˜ë ´
        mean_value = np.mean(matrix)
        adjustment_factor = target_variance / current_variance

        adjusted_matrix = mean_value + (matrix - mean_value) * adjustment_factor
        return adjusted_matrix

    return matrix

def apply_cooperation_weighting(matrix: np.ndarray,
                              synergy_multiplier: float,
                              individual_penalty: float) -> np.ndarray:
    """í˜‘ë ¥ ê°€ì¤‘ì¹˜ ì ìš©"""

    # ë§¤íŠ¸ë¦­ìŠ¤ì˜ í˜‘ë ¥ ê´€ë ¨ ìš”ì†Œ ê°•í™”
    cooperation_enhanced = matrix.copy()

    # [1, 0], [1, 1] ìš”ì†ŒëŠ” í˜‘ë ¥ ê´€ë ¨ (ì‹œë„ˆì§€ ê°•í™”)
    cooperation_enhanced[1, 0] *= synergy_multiplier
    cooperation_enhanced[1, 1] *= synergy_multiplier

    # [0, 0], [0, 1] ìš”ì†ŒëŠ” ê°œì¸ ê´€ë ¨ (í˜ë„í‹° ì ìš©)
    cooperation_enhanced[0, 0] *= (1.0 - individual_penalty)
    cooperation_enhanced[0, 1] *= (1.0 - individual_penalty)

    return cooperation_enhanced

class AutoencoderBalancingEngine:
    """ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± ì—”ì§„"""

    def __init__(self):
        self.encoder_weights = self.initialize_encoder_weights()
        self.decoder_weights = self.initialize_decoder_weights()
        self.training_history = []

    def balance_game(self, input_data: GameBalanceInput) -> np.ndarray:
        """ê²Œì„ ë°¸ëŸ°ì‹± ì‹¤í–‰"""

        # 1. ì¸ì½”ë”©: ë³µì¡í•œ ìƒíƒœ â†’ ì ì¬ ê³µê°„
        latent_vector = encode_to_latent_space(input_data)

        # 2. ì ì¬ ê³µê°„ ë¶„ì„
        print(visualize_latent_space(latent_vector))

        # 3. ë””ì½”ë”©: ì ì¬ ê³µê°„ â†’ ìµœì¢… ë§¤íŠ¸ë¦­ìŠ¤
        final_matrices = decode_to_final_matrices(
            latent_vector, input_data.designer_matrices
        )

        # 4. ê²°ê³¼ ê²€ì¦
        validation_score = self.validate_output(final_matrices, input_data)

        # 5. í•™ìŠµ ë°ì´í„° ì €ì¥
        self.training_history.append({
            'input': input_data,
            'latent': latent_vector,
            'output': final_matrices,
            'validation': validation_score
        })

        return final_matrices

    def validate_output(self, final_matrices: np.ndarray,
                       input_data: GameBalanceInput) -> float:
        """ì¶œë ¥ ê²€ì¦"""

        # 1. í”„ë¡œë² ë‹ˆìš°ìŠ¤ ë…¸ë¦„ ë¶„ì‚° ì²´í¬
        norms = [np.linalg.norm(matrix, 'fro') for matrix in final_matrices]
        norm_variance = np.var(norms)

        # 2. ë””ìì´ë„ˆ ì˜ë„ì™€ì˜ ì°¨ì´
        designer_diff = np.mean([
            np.linalg.norm(final - designer, 'fro')
            for final, designer in zip(final_matrices, input_data.designer_matrices)
        ])

        # 3. ì¢…í•© ì ìˆ˜
        balance_score = 1.0 / (1.0 + norm_variance)
        fidelity_score = 1.0 / (1.0 + designer_diff)

        overall_score = (balance_score + fidelity_score) / 2

        return overall_score

    def continuous_learning(self, player_feedback: Dict):
        """ì§€ì†ì  í•™ìŠµ"""

        if len(self.training_history) > 0:
            latest_session = self.training_history[-1]

            # í”Œë ˆì´ì–´ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •
            if player_feedback['satisfaction'] > 0.8:
                # ì„±ê³µì ì¸ ë°¸ëŸ°ì‹± â†’ ê°€ì¤‘ì¹˜ ê°•í™”
                self.reinforce_weights(latest_session)
            elif player_feedback['satisfaction'] < 0.4:
                # ì‹¤íŒ¨í•œ ë°¸ëŸ°ì‹± â†’ ê°€ì¤‘ì¹˜ ì¡°ì •
                self.adjust_weights(latest_session, player_feedback)

    def generate_balance_report(self) -> str:
        """ë°¸ëŸ°ì‹± ë¦¬í¬íŠ¸ ìƒì„±"""

        if not self.training_history:
            return "í•™ìŠµ ë°ì´í„° ì—†ìŒ"

        recent_scores = [session['validation'] for session in self.training_history[-10:]]
        avg_score = np.mean(recent_scores)
        improvement = recent_scores[-1] - recent_scores[0] if len(recent_scores) > 1 else 0

        report = f"""
        ğŸ“Š ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± ë¦¬í¬íŠ¸:

        ğŸ¯ ìµœê·¼ ì„±ëŠ¥:
        - í‰ê·  ë°¸ëŸ°ìŠ¤ ì ìˆ˜: {avg_score:.3f}
        - ê°œì„ ë„: {improvement:+.3f}
        - ì´ ì„¸ì…˜ ìˆ˜: {len(self.training_history)}

        ğŸ§  í•™ìŠµ ìƒíƒœ:
        - ì¸ì½”ë” ì•ˆì •ì„±: {'ë†’ìŒ' if avg_score > 0.8 else 'ë³´í†µ' if avg_score > 0.6 else 'ë‚®ìŒ'}
        - ë””ì½”ë” ì •í™•ì„±: {'ë†’ìŒ' if improvement > 0 else 'ë³´í†µ' if improvement > -0.1 else 'ë‚®ìŒ'}
        """

        return report

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì˜¤í† ì¸ì½”ë” ì—”ì§„ ì´ˆê¸°í™”
    engine = AutoencoderBalancingEngine()

    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    input_data = GameBalanceInput()
    # ... ë°ì´í„° ì„¤ì • ...

    # ë°¸ëŸ°ì‹± ì‹¤í–‰
    final_matrices = engine.balance_game(input_data)

    print("ğŸ® ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± ì™„ë£Œ!")
    print(engine.generate_balance_report())
```

## ğŸ“ ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì‹œìŠ¤í…œ

### í•™ìŠµ ë°ì´í„° ìƒì„±
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

class GameBalanceDataset(Dataset):
    """ê²Œì„ ë°¸ëŸ°ìŠ¤ í•™ìŠµ ë°ì´í„°ì…‹"""

    def __init__(self, num_samples: int = 10000):
        self.samples = []
        self.generate_synthetic_data(num_samples)

    def generate_synthetic_data(self, num_samples: int):
        """í•©ì„± í•™ìŠµ ë°ì´í„° ìƒì„±"""

        for _ in range(num_samples):
            # 1. ëœë¤ ë””ìì´ë„ˆ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            designer_matrices = self.generate_random_designer_matrices()

            # 2. ëœë¤ ê²Œì„ ìƒíƒœ ìƒì„±
            game_state = self.generate_random_game_state()

            # 3. ëª©í‘œ ì ì¬ ë²¡í„° ê³„ì‚° (ê°ë… í•™ìŠµìš©)
            target_latent = self.calculate_ideal_latent(designer_matrices, game_state)

            # 4. ëª©í‘œ ì¶œë ¥ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            target_output = self.calculate_ideal_output(designer_matrices, target_latent)

            sample = {
                'input_matrices': designer_matrices.flatten(),  # 162Ã—4 = 648ì°¨ì›
                'game_state': self.encode_game_state(game_state),  # 10ì°¨ì›
                'target_latent': target_latent,  # 3ì°¨ì›
                'target_output': target_output.flatten()  # 648ì°¨ì›
            }

            self.samples.append(sample)

    def generate_random_designer_matrices(self) -> np.ndarray:
        """ëœë¤ ë””ìì´ë„ˆ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        matrices = np.zeros((162, 2, 2))

        # 18ê°œ ì¢…ì¡±ì˜ ê¸°ë³¸ ë§¤íŠ¸ë¦­ìŠ¤ ì‚¬ìš©
        race_matrices = [
            [[1.0, 1.0], [1.0, 1.0]],  # human
            [[1.3, 0.7], [1.2, 0.8]],  # elven
            # ... ë‚˜ë¨¸ì§€ 16ê°œ ì¢…ì¡±
        ]

        for race_idx in range(18):
            base_matrix = np.array(race_matrices[race_idx % len(race_matrices)])

            for tower_idx in range(9):
                # íƒ€ì›Œë³„ ë³€í˜• ì ìš©
                variation = np.random.normal(1.0, 0.1, (2, 2))
                matrices[race_idx * 9 + tower_idx] = base_matrix * variation

        return matrices

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        return (
            torch.FloatTensor(sample['input_matrices']),
            torch.FloatTensor(sample['game_state']),
            torch.FloatTensor(sample['target_latent']),
            torch.FloatTensor(sample['target_output'])
        )

class BalanceAutoencoder(nn.Module):
    """ê²Œì„ ë°¸ëŸ°ìŠ¤ ì˜¤í† ì¸ì½”ë” ì‹ ê²½ë§"""

    def __init__(self):
        super(BalanceAutoencoder, self).__init__()

        # ì¸ì½”ë”: 658ì°¨ì› â†’ 3ì°¨ì›
        self.encoder = nn.Sequential(
            nn.Linear(658, 256),  # 648(ë§¤íŠ¸ë¦­ìŠ¤) + 10(ê²Œì„ìƒíƒœ)
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 16),
            nn.ReLU(),
            nn.Linear(16, 3),  # ì ì¬ ê³µê°„
            nn.Tanh()  # [-1, 1] ë²”ìœ„ë¡œ ì œí•œ
        )

        # ë””ì½”ë”: 3ì°¨ì› + 648ì°¨ì›(ì›ë³¸) â†’ 648ì°¨ì›
        self.decoder = nn.Sequential(
            nn.Linear(651, 256),  # 3(ì ì¬) + 648(ì›ë³¸)
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 648),  # ìµœì¢… ë§¤íŠ¸ë¦­ìŠ¤
            nn.Sigmoid()  # [0, 2] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ í•„ìš”
        )

    def forward(self, input_matrices, game_state):
        # ì¸ì½”ë”©
        encoder_input = torch.cat([input_matrices, game_state], dim=1)
        latent = self.encoder(encoder_input)

        # ë””ì½”ë”©
        decoder_input = torch.cat([latent, input_matrices], dim=1)
        output = self.decoder(decoder_input) * 2.0  # [0, 2] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§

        return latent, output

class BalanceTrainer:
    """ì˜¤í† ì¸ì½”ë” í•™ìŠµê¸°"""

    def __init__(self, model: BalanceAutoencoder):
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=0.001)
        self.latent_criterion = nn.MSELoss()
        self.output_criterion = nn.MSELoss()
        self.training_history = []

    def train_epoch(self, dataloader: DataLoader) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()

        total_latent_loss = 0
        total_output_loss = 0
        total_samples = 0

        for batch_idx, (input_matrices, game_state, target_latent, target_output) in enumerate(dataloader):
            self.optimizer.zero_grad()

            # ìˆœì „íŒŒ
            pred_latent, pred_output = self.model(input_matrices, game_state)

            # ì†ì‹¤ ê³„ì‚°
            latent_loss = self.latent_criterion(pred_latent, target_latent)
            output_loss = self.output_criterion(pred_output, target_output)

            # ì´ ì†ì‹¤ (ê°€ì¤‘ í•©)
            total_loss = latent_loss * 0.3 + output_loss * 0.7

            # ì—­ì „íŒŒ
            total_loss.backward()
            self.optimizer.step()

            # í†µê³„ ì—…ë°ì´íŠ¸
            batch_size = input_matrices.size(0)
            total_latent_loss += latent_loss.item() * batch_size
            total_output_loss += output_loss.item() * batch_size
            total_samples += batch_size

        return {
            'latent_loss': total_latent_loss / total_samples,
            'output_loss': total_output_loss / total_samples,
            'total_loss': (total_latent_loss + total_output_loss) / total_samples
        }

    def train(self, num_epochs: int = 100, batch_size: int = 32):
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""

        # ë°ì´í„°ì…‹ ì¤€ë¹„
        dataset = GameBalanceDataset(num_samples=10000)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        print("ğŸ“ ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì‹œì‘...")

        for epoch in range(num_epochs):
            # í•™ìŠµ
            train_metrics = self.train_epoch(dataloader)

            # ê¸°ë¡
            self.training_history.append(train_metrics)

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{num_epochs}:")
                print(f"  Latent Loss: {train_metrics['latent_loss']:.4f}")
                print(f"  Output Loss: {train_metrics['output_loss']:.4f}")
                print(f"  Total Loss: {train_metrics['total_loss']:.4f}")

        print("âœ… í•™ìŠµ ì™„ë£Œ!")

    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_history': self.training_history
        }, path)

    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.training_history = checkpoint['training_history']

# ì‹¤ì œ ê²Œì„ í†µí•©
class RealTimeBalancer:
    """ì‹¤ì‹œê°„ ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì„œ"""

    def __init__(self, model_path: str):
        self.model = BalanceAutoencoder()
        self.load_trained_model(model_path)
        self.model.eval()

    def load_trained_model(self, path: str):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])

    def balance_real_game(self, game_data: Dict) -> np.ndarray:
        """ì‹¤ì œ ê²Œì„ ë°¸ëŸ°ì‹±"""

        with torch.no_grad():
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            input_matrices = torch.FloatTensor(game_data['designer_matrices'].flatten()).unsqueeze(0)
            game_state = torch.FloatTensor(self.encode_game_state(game_data['current_state'])).unsqueeze(0)

            # ì˜¤í† ì¸ì½”ë” ì‹¤í–‰
            latent, output = self.model(input_matrices, game_state)

            # ê²°ê³¼ ë³€í™˜
            final_matrices = output.squeeze().numpy().reshape(162, 2, 2)
            latent_vector = latent.squeeze().numpy()

            # í•´ì„
            interpretation = self.interpret_latent(latent_vector)

            return {
                'final_matrices': final_matrices,
                'latent_state': latent_vector,
                'interpretation': interpretation
            }

    def interpret_latent(self, latent_vector: np.ndarray) -> Dict:
        """ì ì¬ ë²¡í„° í•´ì„"""
        difficulty = latent_vector[0]
        balance_target = latent_vector[1]
        cooperation_weight = latent_vector[2]

        return {
            'difficulty_adjustment': f"{difficulty:+.2f} ({'ì–´ë µê²Œ' if difficulty > 0 else 'ì‰½ê²Œ'})",
            'balance_strictness': f"{balance_target:.2f} ({'ì—„ê²©' if balance_target > 0.7 else 'ê´€ëŒ€'})",
            'cooperation_emphasis': f"{cooperation_weight:.2f} ({'í˜‘ë ¥ ì¤‘ì‹¬' if cooperation_weight > 0.7 else 'ê°œì¸ ì¤‘ì‹¬'})"
        }

# ì„±ëŠ¥ ê²€ì¦ ì‹œìŠ¤í…œ
class PerformanceValidator:
    """ì˜¤í† ì¸ì½”ë” ì„±ëŠ¥ ê²€ì¦ê¸°"""

    def __init__(self, model: BalanceAutoencoder):
        self.model = model

    def validate_reconstruction_quality(self, test_data: GameBalanceDataset) -> Dict:
        """ì¬êµ¬ì„± í’ˆì§ˆ ê²€ì¦"""

        self.model.eval()
        total_mse = 0
        total_samples = 0

        with torch.no_grad():
            for input_matrices, game_state, _, target_output in test_data:
                input_matrices = input_matrices.unsqueeze(0)
                game_state = game_state.unsqueeze(0)

                _, pred_output = self.model(input_matrices, game_state)

                mse = nn.MSELoss()(pred_output, target_output.unsqueeze(0))
                total_mse += mse.item()
                total_samples += 1

        avg_mse = total_mse / total_samples
        reconstruction_quality = 1.0 / (1.0 + avg_mse)

        return {
            'average_mse': avg_mse,
            'reconstruction_quality': reconstruction_quality,
            'quality_grade': 'A' if reconstruction_quality > 0.9 else 'B' if reconstruction_quality > 0.8 else 'C'
        }

    def validate_latent_space_consistency(self, test_data: GameBalanceDataset) -> Dict:
        """ì ì¬ ê³µê°„ ì¼ê´€ì„± ê²€ì¦"""

        latent_vectors = []

        self.model.eval()
        with torch.no_grad():
            for input_matrices, game_state, _, _ in test_data:
                input_matrices = input_matrices.unsqueeze(0)
                game_state = game_state.unsqueeze(0)

                latent, _ = self.model(input_matrices, game_state)
                latent_vectors.append(latent.squeeze().numpy())

        latent_array = np.array(latent_vectors)

        # ê° ì°¨ì›ì˜ ë¶„í¬ ë¶„ì„
        dimension_stats = {}
        for i in range(3):
            dimension_stats[f'dim_{i}'] = {
                'mean': np.mean(latent_array[:, i]),
                'std': np.std(latent_array[:, i]),
                'range': (np.min(latent_array[:, i]), np.max(latent_array[:, i]))
            }

        return {
            'latent_distribution': dimension_stats,
            'space_utilization': np.std(latent_array),  # ì ì¬ ê³µê°„ í™œìš©ë„
            'consistency_score': 1.0 / (1.0 + np.var(latent_array))
        }

# ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰
def main_autoencoder_training():
    """ì˜¤í† ì¸ì½”ë” ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""

    print("ğŸ§  Defense Allies ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± ì‹œìŠ¤í…œ")
    print("=" * 50)

    # 1. ëª¨ë¸ ì´ˆê¸°í™”
    model = BalanceAutoencoder()
    trainer = BalanceTrainer(model)

    # 2. í•™ìŠµ ì‹¤í–‰
    trainer.train(num_epochs=100, batch_size=32)

    # 3. ëª¨ë¸ ì €ì¥
    trainer.save_model('defense_allies_autoencoder.pth')

    # 4. ì„±ëŠ¥ ê²€ì¦
    test_dataset = GameBalanceDataset(num_samples=1000)
    validator = PerformanceValidator(model)

    reconstruction_results = validator.validate_reconstruction_quality(test_dataset)
    latent_results = validator.validate_latent_space_consistency(test_dataset)

    print("\nğŸ“Š ì„±ëŠ¥ ê²€ì¦ ê²°ê³¼:")
    print(f"ì¬êµ¬ì„± í’ˆì§ˆ: {reconstruction_results['quality_grade']} ({reconstruction_results['reconstruction_quality']:.3f})")
    print(f"ì ì¬ ê³µê°„ ì¼ê´€ì„±: {latent_results['consistency_score']:.3f}")

    # 5. ì‹¤ì‹œê°„ ë°¸ëŸ°ì„œ í…ŒìŠ¤íŠ¸
    realtime_balancer = RealTimeBalancer('defense_allies_autoencoder.pth')

    # í…ŒìŠ¤íŠ¸ ê²Œì„ ë°ì´í„°
    test_game_data = {
        'designer_matrices': np.random.rand(162, 2, 2),
        'current_state': {
            'player_count': 4,
            'game_progress': 0.6,
            'average_skill': 0.8,
            'cooperation_level': 0.7
        }
    }

    balance_result = realtime_balancer.balance_real_game(test_game_data)

    print("\nğŸ® ì‹¤ì‹œê°„ ë°¸ëŸ°ì‹± í…ŒìŠ¤íŠ¸:")
    print(f"ì ì¬ ìƒíƒœ: {balance_result['latent_state']}")
    print("í•´ì„:")
    for key, value in balance_result['interpretation'].items():
        print(f"  {key}: {value}")

    print("\nâœ… ì˜¤í† ì¸ì½”ë” ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")

if __name__ == "__main__":
    main_autoencoder_training()
```

## ğŸ† ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹±ì˜ í˜ì‹ ì  ê°€ì¹˜

### ì„¸ê³„ ìµœì´ˆì˜ ì„±ê³¼
1. **ê²Œì„ ë°¸ëŸ°ì‹±ì— ì˜¤í† ì¸ì½”ë” ì ìš©**: ê¸°ì¡´ì— ì—†ë˜ ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ë²•
2. **ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ**: í”Œë ˆì´ì–´ í”¼ë“œë°±ìœ¼ë¡œ ì§€ì†ì  ê°œì„ 
3. **3ì°¨ì› ì ì¬ ê³µê°„**: ë³µì¡í•œ ê²Œì„ ìƒíƒœë¥¼ ì§ê´€ì ìœ¼ë¡œ ì••ì¶•
4. **ë””ìì´ë„ˆ ì˜ë„ ë³´ì¡´**: ì›ë³¸ ì„¤ê³„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìµœì í™”

### ê¸°ìˆ ì  ìš°ìˆ˜ì„±
```yaml
ì…ë ¥ ì°¨ì›: 658ì°¨ì› (648 ë§¤íŠ¸ë¦­ìŠ¤ + 10 ê²Œì„ìƒíƒœ)
ì ì¬ ì°¨ì›: 3ì°¨ì› (ë‚œì´ë„, ë°¸ëŸ°ìŠ¤, í˜‘ë ¥)
ì¶œë ¥ ì°¨ì›: 648ì°¨ì› (162ê°œ íƒ€ì›Œ Ã— 4 ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì†Œ)
ì••ì¶•ë¥ : 99.5% (658 â†’ 3 â†’ 648)
```

### ì‹¤ìš©ì  ì¥ì 
1. **ì™„ì „ ìë™í™”**: ìˆ˜ë™ ë°¸ëŸ°ì‹± ì‘ì—… ë¶ˆí•„ìš”
2. **ì‹¤ì‹œê°„ ì ì‘**: ê²Œì„ ì¤‘ ì¦‰ì‹œ ì¡°ì •
3. **í•™ìŠµ ëŠ¥ë ¥**: í”Œë ˆì´ì–´ ë°ì´í„°ë¡œ ì§€ì† ê°œì„ 
4. **í•´ì„ ê°€ëŠ¥ì„±**: ì ì¬ ê³µê°„ì˜ ëª…í™•í•œ ì˜ë¯¸

**Defense AlliesëŠ” ì´ì œ AIê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²Œì„ì„ ë°¸ëŸ°ì‹±í•˜ëŠ” ì„¸ê³„ ìµœì´ˆì˜ ê²Œì„ì´ ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸ¤–ğŸ®

---

**ë‹¤ìŒ ë‹¨ê³„**: ì‹¤ì œ í”Œë ˆì´ì–´ ë°ì´í„° ìˆ˜ì§‘ ë° ì˜¤í† ì¸ì½”ë” ì‹¤ì „ ë°°í¬
