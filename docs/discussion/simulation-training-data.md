# Defense Allies ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ

## ğŸ“‹ ë¬¸ì„œ ì •ë³´
- **ì‘ì„±ì¼**: 2024ë…„
- **ë²„ì „**: v1.0
- **ëª©ì **: ì˜¤í† ì¸ì½”ë” í•™ìŠµì„ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë°ì´í„° ìƒì„±
- **ê¸°ë°˜**: [ì˜¤í† ì¸ì½”ë” ë°¸ëŸ°ì‹± ì‹œìŠ¤í…œ](autoencoder-balancing-system.md)

## ğŸ¯ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì „ëµ

### í•µì‹¬ ì•„ì´ë””ì–´
```yaml
ì‹¤ì œ ê²Œì„ í”Œë ˆì´ ì—†ì´ë„ ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„° í™•ë³´:

1. ê²Œì„ ë©”ì»¤ë‹ˆì¦˜ ì‹œë®¬ë ˆì´ì…˜
   - íƒ€ì›Œ vs ì  ì „íˆ¬ ì‹œë®¬ë ˆì´ì…˜
   - ì¢…ì¡± ê°„ ì‹œë„ˆì§€ íš¨ê³¼ ê³„ì‚°
   - í™˜ê²½ ë³€ìˆ˜ ì˜í–¥ë„ ì¸¡ì •

2. í”Œë ˆì´ì–´ í–‰ë™ ëª¨ë¸ë§
   - ë‹¤ì–‘í•œ ìŠ¤í‚¬ ë ˆë²¨ í”Œë ˆì´ì–´ ì‹œë®¬ë ˆì´ì…˜
   - í˜‘ë ¥ íŒ¨í„´ ëª¨ë¸ë§
   - ì „ëµ ì„ íƒ í™•ë¥  ë¶„í¬

3. ë°¸ëŸ°ìŠ¤ ê²°ê³¼ ì˜ˆì¸¡
   - ìŠ¹ë¥  ì‹œë®¬ë ˆì´ì…˜
   - í”Œë ˆì´ì–´ ë§Œì¡±ë„ ì˜ˆì¸¡
   - ê²Œì„ ì¬ë¯¸ ì§€ìˆ˜ ê³„ì‚°
```

## ğŸ® ê²Œì„ ë©”ì»¤ë‹ˆì¦˜ ì‹œë®¬ë ˆì´í„°

### ì „íˆ¬ ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„
```python
import numpy as np
import random
from typing import Dict, List, Tuple
from dataclasses import dataclass

@dataclass
class Enemy:
    """ì  ìœ ë‹› ì •ì˜"""
    hp: float
    armor: float
    speed: float
    reward: int

@dataclass
class Tower:
    """íƒ€ì›Œ ì •ì˜"""
    power_matrix: np.ndarray
    cost: Dict[str, int]
    range: float
    attack_speed: float

class CombatSimulator:
    """ì „íˆ¬ ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„"""

    def __init__(self):
        self.enemy_waves = self.generate_enemy_waves()
        self.base_hp = 100

    def generate_enemy_waves(self) -> List[List[Enemy]]:
        """ì  ì›¨ì´ë¸Œ ìƒì„±"""
        waves = []

        for wave_num in range(1, 21):  # 20ì›¨ì´ë¸Œ
            wave_enemies = []
            enemy_count = 10 + wave_num * 2

            for i in range(enemy_count):
                # ì›¨ì´ë¸Œê°€ ì§„í–‰ë ìˆ˜ë¡ ê°•í•´ì§
                hp = 50 + wave_num * 10 + random.uniform(-10, 10)
                armor = wave_num * 2 + random.uniform(-2, 2)
                speed = 1.0 + random.uniform(-0.2, 0.2)
                reward = 10 + wave_num

                enemy = Enemy(hp, armor, speed, reward)
                wave_enemies.append(enemy)

            waves.append(wave_enemies)

        return waves

    def simulate_tower_effectiveness(self, tower: Tower, wave_index: int) -> Dict[str, float]:
        """íŠ¹ì • ì›¨ì´ë¸Œì—ì„œ íƒ€ì›Œ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜"""

        wave = self.enemy_waves[wave_index]

        # íƒ€ì›Œ ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ì‹¤ì œ ëŠ¥ë ¥ì¹˜ ì¶”ì¶œ
        offensive_power = np.mean([tower.power_matrix[0, 0], tower.power_matrix[1, 0]])
        defensive_power = np.mean([tower.power_matrix[0, 1], tower.power_matrix[1, 1]])

        # ì „íˆ¬ ì‹œë®¬ë ˆì´ì…˜
        kills = 0
        damage_dealt = 0
        survival_time = 0

        for enemy in wave:
            # íƒ€ì›Œê°€ ì ì„ ê³µê²©í•  ìˆ˜ ìˆëŠ”ì§€ ê³„ì‚°
            if self.can_attack(tower, enemy):
                # ë°ë¯¸ì§€ ê³„ì‚°
                base_damage = offensive_power * 50  # ê¸°ë³¸ ë°ë¯¸ì§€
                actual_damage = max(1, base_damage - enemy.armor)

                # ì  ì²˜ì¹˜ ì‹œê°„ ê³„ì‚°
                time_to_kill = enemy.hp / actual_damage

                if time_to_kill <= 10:  # 10ì´ˆ ë‚´ ì²˜ì¹˜ ê°€ëŠ¥
                    kills += 1
                    damage_dealt += enemy.hp

                survival_time += min(time_to_kill, 10)

        # íš¨ê³¼ì„± ì§€í‘œ ê³„ì‚°
        kill_rate = kills / len(wave)
        damage_efficiency = damage_dealt / (tower.cost['gold'] + tower.cost['mana'])
        survival_score = survival_time / (len(wave) * 10)

        return {
            'kill_rate': kill_rate,
            'damage_efficiency': damage_efficiency,
            'survival_score': survival_score,
            'overall_effectiveness': (kill_rate + damage_efficiency + survival_score) / 3
        }

    def can_attack(self, tower: Tower, enemy: Enemy) -> bool:
        """íƒ€ì›Œê°€ ì ì„ ê³µê²©í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨"""
        # ê°„ë‹¨í•œ ê±°ë¦¬ ê¸°ë°˜ íŒë‹¨ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡)
        return random.random() < 0.8  # 80% í™•ë¥ ë¡œ ê³µê²© ê°€ëŠ¥

class SynergySimulator:
    """ì‹œë„ˆì§€ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜"""

    def __init__(self):
        self.synergy_matrix = self.load_synergy_matrix()

    def load_synergy_matrix(self) -> np.ndarray:
        """18Ã—18 ì‹œë„ˆì§€ ë§¤íŠ¸ë¦­ìŠ¤ ë¡œë“œ"""
        # ì´ì „ì— ì •ì˜í•œ ì‹œë„ˆì§€ ë§¤íŠ¸ë¦­ìŠ¤ ì‚¬ìš©
        return np.random.uniform(0.5, 1.5, (18, 18))

    def simulate_team_synergy(self, team_composition: List[str]) -> Dict[str, float]:
        """íŒ€ ì‹œë„ˆì§€ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜"""

        if len(team_composition) <= 1:
            return {'synergy_bonus': 1.0, 'cooperation_effectiveness': 0.0}

        # ëª¨ë“  ì¢…ì¡± ìŒì˜ ì‹œë„ˆì§€ ê³„ì‚°
        total_synergy = 0
        pair_count = 0

        race_indices = [self.get_race_index(race) for race in team_composition]

        for i, race1_idx in enumerate(race_indices):
            for race2_idx in race_indices[i+1:]:
                synergy_value = self.synergy_matrix[race1_idx, race2_idx]
                total_synergy += synergy_value
                pair_count += 1

        avg_synergy = total_synergy / pair_count if pair_count > 0 else 1.0

        # í˜‘ë ¥ íš¨ê³¼ì„± ê³„ì‚° (íŒ€ í¬ê¸°ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤)
        cooperation_effectiveness = min(len(team_composition) / 4.0, 1.0)

        return {
            'synergy_bonus': avg_synergy,
            'cooperation_effectiveness': cooperation_effectiveness,
            'team_power_multiplier': avg_synergy * (1 + cooperation_effectiveness * 0.5)
        }

    def get_race_index(self, race_name: str) -> int:
        """ì¢…ì¡± ì´ë¦„ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
        race_names = [
            'human_alliance', 'elven_kingdom', 'dwarven_clan', 'orc_tribe',
            'undead_legion', 'dragon_clan', 'mechanical_empire', 'angel_legion',
            'elemental_spirits', 'ocean_empire', 'plant_kingdom', 'insect_swarm',
            'crystal_beings', 'time_weavers', 'shadow_clan', 'cosmic_empire',
            'viral_collective', 'harmony_tribe'
        ]
        return race_names.index(race_name) if race_name in race_names else 0

class EnvironmentSimulator:
    """í™˜ê²½ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜"""

    def __init__(self):
        self.environment_effects = self.load_environment_effects()

    def load_environment_effects(self) -> Dict:
        """í™˜ê²½ íš¨ê³¼ ë§¤íŠ¸ë¦­ìŠ¤ ë¡œë“œ"""
        return {
            'time': {
                'day': {'light_bonus': 1.2, 'dark_penalty': 0.8},
                'night': {'light_bonus': 0.8, 'dark_penalty': 1.2},
                'dawn': {'light_bonus': 1.1, 'dark_penalty': 0.9},
                'dusk': {'light_bonus': 0.9, 'dark_penalty': 1.1}
            },
            'weather': {
                'clear': {'visibility': 1.0, 'magic_efficiency': 1.0},
                'rain': {'visibility': 0.8, 'magic_efficiency': 1.2},
                'storm': {'visibility': 0.6, 'magic_efficiency': 1.4},
                'snow': {'visibility': 0.7, 'magic_efficiency': 0.9}
            },
            'terrain': {
                'plain': {'movement': 1.0, 'defense': 1.0},
                'forest': {'movement': 0.8, 'defense': 1.3},
                'mountain': {'movement': 0.6, 'defense': 1.5},
                'desert': {'movement': 0.9, 'defense': 0.8}
            }
        }

    def simulate_environment_impact(self, race_name: str, environment: Dict[str, str]) -> Dict[str, float]:
        """íŠ¹ì • í™˜ê²½ì—ì„œ ì¢…ì¡± ì˜í–¥ë„ ì‹œë®¬ë ˆì´ì…˜"""

        # ì¢…ì¡±ë³„ í™˜ê²½ ì¹œí™”ë„
        race_affinities = {
            'elven_kingdom': {'forest': 1.5, 'nature_magic': 1.3},
            'dwarven_clan': {'mountain': 1.4, 'underground': 1.3},
            'dragon_clan': {'mountain': 1.2, 'fire_magic': 1.4},
            'undead_legion': {'night': 1.3, 'dark_magic': 1.4},
            'angel_legion': {'day': 1.3, 'light_magic': 1.4},
            'ocean_empire': {'rain': 1.4, 'water_magic': 1.5},
            # ... ë‚˜ë¨¸ì§€ ì¢…ì¡±ë“¤
        }

        base_effectiveness = 1.0

        # ì‹œê°„ íš¨ê³¼
        time_effect = self.environment_effects['time'][environment['time']]
        if race_name in ['angel_legion', 'human_alliance']:
            base_effectiveness *= time_effect['light_bonus']
        elif race_name in ['undead_legion', 'shadow_clan']:
            base_effectiveness *= time_effect['dark_penalty']

        # ë‚ ì”¨ íš¨ê³¼
        weather_effect = self.environment_effects['weather'][environment['weather']]
        if race_name in ['elemental_spirits', 'crystal_beings']:
            base_effectiveness *= weather_effect['magic_efficiency']

        # ì§€í˜• íš¨ê³¼
        terrain_effect = self.environment_effects['terrain'][environment['terrain']]
        if race_name in race_affinities and environment['terrain'] in race_affinities[race_name]:
            base_effectiveness *= race_affinities[race_name][environment['terrain']]

        return {
            'environment_multiplier': base_effectiveness,
            'adaptation_score': min(base_effectiveness, 2.0),  # ìµœëŒ€ 2ë°°
            'penalty_score': max(0.5, base_effectiveness)      # ìµœì†Œ 0.5ë°°
        }

## ğŸ¤– í”Œë ˆì´ì–´ í–‰ë™ ì‹œë®¬ë ˆì´í„°

class PlayerBehaviorSimulator:
    """í”Œë ˆì´ì–´ í–‰ë™ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜"""

    def __init__(self):
        self.skill_levels = ['beginner', 'intermediate', 'advanced', 'expert']
        self.cooperation_styles = ['solo', 'casual', 'coordinated', 'competitive']

    def simulate_player_decisions(self, game_state: Dict, player_profile: Dict) -> Dict:
        """í”Œë ˆì´ì–´ ì˜ì‚¬ê²°ì • ì‹œë®¬ë ˆì´ì…˜"""

        skill_level = player_profile['skill_level']
        cooperation_style = player_profile['cooperation_style']

        # ìŠ¤í‚¬ ë ˆë²¨ë³„ ì˜ì‚¬ê²°ì • í’ˆì§ˆ
        decision_quality = {
            'beginner': 0.3,
            'intermediate': 0.6,
            'advanced': 0.8,
            'expert': 0.95
        }[skill_level]

        # í˜‘ë ¥ ìŠ¤íƒ€ì¼ë³„ íŒ€ì›Œí¬ ì ìˆ˜
        teamwork_score = {
            'solo': 0.2,
            'casual': 0.5,
            'coordinated': 0.8,
            'competitive': 0.9
        }[cooperation_style]

        # íƒ€ì›Œ ì„ íƒ ì‹œë®¬ë ˆì´ì…˜
        tower_choices = self.simulate_tower_selection(
            game_state, decision_quality, teamwork_score
        )

        # ìì› ê´€ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        resource_efficiency = self.simulate_resource_management(
            game_state, decision_quality
        )

        # í˜‘ë ¥ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
        cooperation_actions = self.simulate_cooperation_behavior(
            game_state, teamwork_score
        )

        return {
            'tower_choices': tower_choices,
            'resource_efficiency': resource_efficiency,
            'cooperation_actions': cooperation_actions,
            'overall_performance': (decision_quality + teamwork_score) / 2
        }

    def simulate_tower_selection(self, game_state: Dict, decision_quality: float, teamwork_score: float) -> Dict:
        """íƒ€ì›Œ ì„ íƒ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜"""

        # ê²Œì„ ì§„í–‰ë„ì— ë”°ë¥¸ íƒ€ì›Œ ì„ íƒ
        progress = game_state.get('progress', 0.0)

        if progress < 0.3:  # ì´ˆë°˜
            tower_preference = 'basic' if decision_quality < 0.7 else 'mixed'
        elif progress < 0.7:  # ì¤‘ë°˜
            tower_preference = 'advanced' if decision_quality > 0.5 else 'basic'
        else:  # í›„ë°˜
            tower_preference = 'cooperation' if teamwork_score > 0.6 else 'advanced'

        # ì„ íƒ ë‹¤ì–‘ì„± (ìŠ¤í‚¬ì´ ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì„ íƒ)
        selection_diversity = decision_quality * 0.8 + random.uniform(0, 0.2)

        return {
            'tower_preference': tower_preference,
            'selection_diversity': selection_diversity,
            'strategic_depth': decision_quality * teamwork_score
        }

    def simulate_resource_management(self, game_state: Dict, decision_quality: float) -> Dict:
        """ìì› ê´€ë¦¬ íš¨ìœ¨ì„± ì‹œë®¬ë ˆì´ì…˜"""

        # ê¸°ë³¸ íš¨ìœ¨ì„± + ëœë¤ ìš”ì†Œ
        base_efficiency = decision_quality * 0.8 + random.uniform(0, 0.2)

        # ê²Œì„ ìƒí™©ì— ë”°ë¥¸ ì¡°ì •
        if game_state.get('under_pressure', False):
            # ì••ë°• ìƒí™©ì—ì„œëŠ” íš¨ìœ¨ì„± ê°ì†Œ
            pressure_penalty = (1 - decision_quality) * 0.3
            base_efficiency -= pressure_penalty

        return {
            'gold_efficiency': max(0.1, base_efficiency),
            'mana_efficiency': max(0.1, base_efficiency * 0.9),  # ë§ˆë‚˜ê°€ ì•½ê°„ ë” ì–´ë ¤ì›€
            'timing_accuracy': decision_quality
        }

    def simulate_cooperation_behavior(self, game_state: Dict, teamwork_score: float) -> Dict:
        """í˜‘ë ¥ í–‰ë™ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜"""

        # í˜‘ë ¥ íƒ€ì›Œ ê±´ì„¤ í™•ë¥ 
        coop_tower_probability = teamwork_score * 0.7 + random.uniform(0, 0.3)

        # ìì› ê³µìœ  ì˜í–¥
        resource_sharing = teamwork_score * 0.6 + random.uniform(0, 0.4)

        # ì „ëµ ì¡°ìœ¨ ìˆ˜ì¤€
        strategy_coordination = teamwork_score * 0.8 + random.uniform(0, 0.2)

        return {
            'coop_tower_probability': coop_tower_probability,
            'resource_sharing': resource_sharing,
            'strategy_coordination': strategy_coordination,
            'communication_frequency': teamwork_score
        }

## ğŸ“Š ê²Œì„ ê²°ê³¼ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´í„°

class GameOutcomeSimulator:
    """ê²Œì„ ê²°ê³¼ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜"""

    def __init__(self):
        self.combat_sim = CombatSimulator()
        self.synergy_sim = SynergySimulator()
        self.env_sim = EnvironmentSimulator()
        self.player_sim = PlayerBehaviorSimulator()

    def simulate_full_game(self, game_setup: Dict) -> Dict:
        """ì „ì²´ ê²Œì„ ì‹œë®¬ë ˆì´ì…˜"""

        # ê²Œì„ ì„¤ì • ì¶”ì¶œ
        team_composition = game_setup['team_composition']
        tower_matrices = game_setup['tower_matrices']
        environment = game_setup['environment']
        player_profiles = game_setup['player_profiles']

        # ê° êµ¬ì„± ìš”ì†Œ ì‹œë®¬ë ˆì´ì…˜
        synergy_results = self.synergy_sim.simulate_team_synergy(team_composition)

        environment_results = {}
        for race in team_composition:
            env_result = self.env_sim.simulate_environment_impact(race, environment)
            environment_results[race] = env_result

        player_results = {}
        for i, profile in enumerate(player_profiles):
            player_result = self.player_sim.simulate_player_decisions(
                game_setup, profile
            )
            player_results[f'player_{i}'] = player_result

        # ì „íˆ¬ íš¨ê³¼ì„± ê³„ì‚°
        combat_effectiveness = self.calculate_combat_effectiveness(
            tower_matrices, synergy_results, environment_results
        )

        # ìµœì¢… ê²Œì„ ê²°ê³¼ ì˜ˆì¸¡
        win_probability = self.predict_win_probability(
            combat_effectiveness, synergy_results, player_results
        )

        # í”Œë ˆì´ì–´ ë§Œì¡±ë„ ì˜ˆì¸¡
        satisfaction_score = self.predict_player_satisfaction(
            win_probability, synergy_results, player_results
        )

        return {
            'win_probability': win_probability,
            'satisfaction_score': satisfaction_score,
            'combat_effectiveness': combat_effectiveness,
            'synergy_bonus': synergy_results['synergy_bonus'],
            'cooperation_level': synergy_results['cooperation_effectiveness'],
            'balance_quality': self.calculate_balance_quality(player_results)
        }

    def calculate_combat_effectiveness(self, tower_matrices: List[np.ndarray],
                                     synergy_results: Dict,
                                     environment_results: Dict) -> float:
        """ì „íˆ¬ íš¨ê³¼ì„± ê³„ì‚°"""

        base_power = sum(np.linalg.norm(matrix, 'fro') for matrix in tower_matrices)
        synergy_multiplier = synergy_results['team_power_multiplier']

        # í™˜ê²½ íš¨ê³¼ í‰ê· 
        env_multiplier = np.mean([
            result['environment_multiplier']
            for result in environment_results.values()
        ])

        total_effectiveness = base_power * synergy_multiplier * env_multiplier

        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        return min(total_effectiveness / 100, 1.0)

    def predict_win_probability(self, combat_effectiveness: float,
                               synergy_results: Dict,
                               player_results: Dict) -> float:
        """ìŠ¹ë¦¬ í™•ë¥  ì˜ˆì¸¡"""

        # ì „íˆ¬ë ¥ ê¸°ì—¬ë„ (40%)
        combat_factor = combat_effectiveness * 0.4

        # ì‹œë„ˆì§€ ê¸°ì—¬ë„ (30%)
        synergy_factor = synergy_results['cooperation_effectiveness'] * 0.3

        # í”Œë ˆì´ì–´ ìŠ¤í‚¬ ê¸°ì—¬ë„ (30%)
        avg_performance = np.mean([
            result['overall_performance']
            for result in player_results.values()
        ])
        skill_factor = avg_performance * 0.3

        win_prob = combat_factor + synergy_factor + skill_factor

        # ëœë¤ ìš”ì†Œ ì¶”ê°€ (ê²Œì„ì˜ ë¶ˆí™•ì‹¤ì„±)
        random_factor = random.uniform(-0.1, 0.1)

        return max(0.0, min(1.0, win_prob + random_factor))

    def predict_player_satisfaction(self, win_probability: float,
                                   synergy_results: Dict,
                                   player_results: Dict) -> float:
        """í”Œë ˆì´ì–´ ë§Œì¡±ë„ ì˜ˆì¸¡"""

        # ìŠ¹ë¦¬ ê°€ëŠ¥ì„±ì— ë”°ë¥¸ ë§Œì¡±ë„ (ì ë‹¹í•œ ë„ì „ì´ ìµœê³ )
        if 0.4 <= win_probability <= 0.6:
            win_satisfaction = 1.0  # ìµœì ì˜ ë°¸ëŸ°ìŠ¤
        elif 0.3 <= win_probability <= 0.7:
            win_satisfaction = 0.8  # ì¢‹ì€ ë°¸ëŸ°ìŠ¤
        else:
            win_satisfaction = 0.5  # ë„ˆë¬´ ì‰½ê±°ë‚˜ ì–´ë ¤ì›€

        # í˜‘ë ¥ ì¬ë¯¸ë„
        coop_satisfaction = synergy_results['cooperation_effectiveness']

        # ê°œì¸ ì„±ì·¨ê°
        personal_satisfaction = np.mean([
            result['overall_performance']
            for result in player_results.values()
        ])

        # ê°€ì¤‘ í‰ê· 
        total_satisfaction = (
            win_satisfaction * 0.4 +
            coop_satisfaction * 0.3 +
            personal_satisfaction * 0.3
        )

        return total_satisfaction

    def calculate_balance_quality(self, player_results: Dict) -> float:
        """ë°¸ëŸ°ìŠ¤ í’ˆì§ˆ ê³„ì‚°"""

        performances = [
            result['overall_performance']
            for result in player_results.values()
        ]

        # ì„±ëŠ¥ ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë°¸ëŸ°ìŠ¤
        performance_variance = np.var(performances)
        balance_quality = 1.0 / (1.0 + performance_variance)

        return balance_quality

# ì‚¬ìš© ì˜ˆì‹œ
def generate_simulation_training_data(num_samples: int = 10000) -> List[Dict]:
    """ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° ìƒì„±"""

    simulator = GameOutcomeSimulator()
    training_data = []

    print(f"ğŸ® {num_samples}ê°œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì¤‘...")

    for i in range(num_samples):
        # ëœë¤ ê²Œì„ ì„¤ì • ìƒì„±
        game_setup = generate_random_game_setup()

        # ê²Œì„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        simulation_result = simulator.simulate_full_game(game_setup)

        # í•™ìŠµ ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜
        training_sample = {
            'input_matrices': game_setup['tower_matrices'],
            'game_state': encode_game_setup(game_setup),
            'target_outcome': simulation_result,
            'ideal_adjustments': calculate_ideal_adjustments(simulation_result)
        }

        training_data.append(training_sample)

        if (i + 1) % 1000 == 0:
            print(f"  ì§„í–‰ë¥ : {i+1}/{num_samples} ({(i+1)/num_samples*100:.1f}%)")

    print("âœ… ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    return training_data

def generate_random_game_setup() -> Dict:
    """ëœë¤ ê²Œì„ ì„¤ì • ìƒì„±"""

    # ëœë¤ íŒ€ êµ¬ì„± (2-4ëª…)
    team_size = random.randint(2, 4)
    all_races = [
        'human_alliance', 'elven_kingdom', 'dwarven_clan', 'orc_tribe',
        'undead_legion', 'dragon_clan', 'mechanical_empire', 'angel_legion'
    ]
    team_composition = random.sample(all_races, team_size)

    # ëœë¤ íƒ€ì›Œ ë§¤íŠ¸ë¦­ìŠ¤ (ê°„ë‹¨í™”)
    tower_matrices = [
        np.random.uniform(0.5, 1.5, (2, 2)) for _ in range(team_size * 3)
    ]

    # ëœë¤ í™˜ê²½
    environment = {
        'time': random.choice(['day', 'night', 'dawn', 'dusk']),
        'weather': random.choice(['clear', 'rain', 'storm', 'snow']),
        'terrain': random.choice(['plain', 'forest', 'mountain', 'desert'])
    }

    # ëœë¤ í”Œë ˆì´ì–´ í”„ë¡œí•„
    player_profiles = []
    for _ in range(team_size):
        profile = {
            'skill_level': random.choice(['beginner', 'intermediate', 'advanced', 'expert']),
            'cooperation_style': random.choice(['solo', 'casual', 'coordinated', 'competitive'])
        }
        player_profiles.append(profile)

    return {
        'team_composition': team_composition,
        'tower_matrices': tower_matrices,
        'environment': environment,
        'player_profiles': player_profiles
    }

if __name__ == "__main__":
    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸
    training_data = generate_simulation_training_data(1000)
    print(f"ìƒì„±ëœ í•™ìŠµ ë°ì´í„°: {len(training_data)}ê°œ")

    # ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸
    sample = training_data[0]
    print(f"ìƒ˜í”Œ êµ¬ì¡°: {list(sample.keys())}")
```

## ğŸ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° í’ˆì§ˆ ê²€ì¦

### ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­
```python
class SimulationDataValidator:
    """ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸°"""

    def __init__(self):
        self.quality_thresholds = {
            'diversity_score': 0.8,      # ë°ì´í„° ë‹¤ì–‘ì„±
            'realism_score': 0.7,        # í˜„ì‹¤ì„±
            'balance_coverage': 0.9,     # ë°¸ëŸ°ìŠ¤ ìƒí™© ì»¤ë²„ë¦¬ì§€
            'correlation_strength': 0.6   # ì…ë ¥-ì¶œë ¥ ìƒê´€ê´€ê³„
        }

    def validate_dataset(self, training_data: List[Dict]) -> Dict[str, float]:
        """ì „ì²´ ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦"""

        print("ğŸ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")

        # 1. ë°ì´í„° ë‹¤ì–‘ì„± ê²€ì¦
        diversity_score = self.check_data_diversity(training_data)

        # 2. í˜„ì‹¤ì„± ê²€ì¦
        realism_score = self.check_data_realism(training_data)

        # 3. ë°¸ëŸ°ìŠ¤ ìƒí™© ì»¤ë²„ë¦¬ì§€ ê²€ì¦
        balance_coverage = self.check_balance_coverage(training_data)

        # 4. ì…ë ¥-ì¶œë ¥ ìƒê´€ê´€ê³„ ê²€ì¦
        correlation_strength = self.check_input_output_correlation(training_data)

        # 5. ì¢…í•© í’ˆì§ˆ ì ìˆ˜
        overall_quality = (
            diversity_score * 0.25 +
            realism_score * 0.25 +
            balance_coverage * 0.25 +
            correlation_strength * 0.25
        )

        results = {
            'diversity_score': diversity_score,
            'realism_score': realism_score,
            'balance_coverage': balance_coverage,
            'correlation_strength': correlation_strength,
            'overall_quality': overall_quality,
            'quality_grade': self.get_quality_grade(overall_quality)
        }

        self.print_validation_report(results)
        return results

    def check_data_diversity(self, training_data: List[Dict]) -> float:
        """ë°ì´í„° ë‹¤ì–‘ì„± ê²€ì¦"""

        # íŒ€ êµ¬ì„± ë‹¤ì–‘ì„±
        team_compositions = [sample['game_state']['team_composition'] for sample in training_data]
        unique_compositions = len(set(map(tuple, team_compositions)))
        composition_diversity = unique_compositions / len(training_data)

        # í™˜ê²½ ë‹¤ì–‘ì„±
        environments = [
            f"{sample['game_state']['environment']['time']}_"
            f"{sample['game_state']['environment']['weather']}_"
            f"{sample['game_state']['environment']['terrain']}"
            for sample in training_data
        ]
        unique_environments = len(set(environments))
        environment_diversity = unique_environments / (4 * 4 * 4)  # ìµœëŒ€ 64ê°€ì§€

        # í”Œë ˆì´ì–´ í”„ë¡œí•„ ë‹¤ì–‘ì„±
        skill_levels = [sample['game_state']['avg_skill_level'] for sample in training_data]
        skill_diversity = len(set(skill_levels)) / 4  # 4ê°€ì§€ ìŠ¤í‚¬ ë ˆë²¨

        # ì¢…í•© ë‹¤ì–‘ì„± ì ìˆ˜
        diversity_score = (composition_diversity + environment_diversity + skill_diversity) / 3

        return min(diversity_score, 1.0)

    def check_data_realism(self, training_data: List[Dict]) -> float:
        """ë°ì´í„° í˜„ì‹¤ì„± ê²€ì¦"""

        realism_scores = []

        for sample in training_data:
            outcome = sample['target_outcome']

            # 1. ìŠ¹ë¥  í˜„ì‹¤ì„± (0.1 ~ 0.9 ë²”ìœ„ê°€ í˜„ì‹¤ì )
            win_prob = outcome['win_probability']
            win_realism = 1.0 if 0.1 <= win_prob <= 0.9 else 0.5

            # 2. ë§Œì¡±ë„ í˜„ì‹¤ì„± (ë„ˆë¬´ ê·¹ë‹¨ì ì´ì§€ ì•Šì•„ì•¼ í•¨)
            satisfaction = outcome['satisfaction_score']
            satisfaction_realism = 1.0 if 0.2 <= satisfaction <= 0.9 else 0.5

            # 3. ë°¸ëŸ°ìŠ¤ í’ˆì§ˆ í˜„ì‹¤ì„±
            balance_quality = outcome['balance_quality']
            balance_realism = 1.0 if 0.3 <= balance_quality <= 0.95 else 0.5

            # 4. ìƒê´€ê´€ê³„ í˜„ì‹¤ì„± (ê°•í•œ íŒ€ì´ ë†’ì€ ìŠ¹ë¥ ì„ ê°€ì ¸ì•¼ í•¨)
            combat_eff = outcome['combat_effectiveness']
            correlation_realism = 1.0 if abs(combat_eff - win_prob) < 0.3 else 0.7

            sample_realism = (win_realism + satisfaction_realism +
                            balance_realism + correlation_realism) / 4
            realism_scores.append(sample_realism)

        return np.mean(realism_scores)

    def check_balance_coverage(self, training_data: List[Dict]) -> float:
        """ë°¸ëŸ°ìŠ¤ ìƒí™© ì»¤ë²„ë¦¬ì§€ ê²€ì¦"""

        # ë‹¤ì–‘í•œ ë°¸ëŸ°ìŠ¤ ìƒí™©ì´ ê³¨ê³ ë£¨ í¬í•¨ë˜ì–´ì•¼ í•¨
        win_prob_bins = np.histogram([
            sample['target_outcome']['win_probability']
            for sample in training_data
        ], bins=10, range=(0, 1))[0]

        satisfaction_bins = np.histogram([
            sample['target_outcome']['satisfaction_score']
            for sample in training_data
        ], bins=10, range=(0, 1))[0]

        # ê° êµ¬ê°„ì— ìµœì†Œí•œì˜ ë°ì´í„°ê°€ ìˆì–´ì•¼ í•¨
        min_samples_per_bin = len(training_data) * 0.05  # 5%

        win_coverage = sum(1 for count in win_prob_bins if count >= min_samples_per_bin) / 10
        satisfaction_coverage = sum(1 for count in satisfaction_bins if count >= min_samples_per_bin) / 10

        return (win_coverage + satisfaction_coverage) / 2

    def check_input_output_correlation(self, training_data: List[Dict]) -> float:
        """ì…ë ¥-ì¶œë ¥ ìƒê´€ê´€ê³„ ê²€ì¦"""

        # ê°•í•œ íŒ€ êµ¬ì„± â†’ ë†’ì€ ìŠ¹ë¥  ìƒê´€ê´€ê³„ í™•ì¸
        team_strengths = []
        win_probabilities = []

        for sample in training_data:
            # íŒ€ ê°•ë„ ê³„ì‚° (ë§¤íŠ¸ë¦­ìŠ¤ ë…¸ë¦„ì˜ í•©)
            matrices = sample['input_matrices']
            team_strength = sum(np.linalg.norm(matrix, 'fro') for matrix in matrices)
            team_strengths.append(team_strength)

            win_prob = sample['target_outcome']['win_probability']
            win_probabilities.append(win_prob)

        # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlation = np.corrcoef(team_strengths, win_probabilities)[0, 1]

        return abs(correlation)  # ì ˆëŒ“ê°’ (ì–‘ì˜ ìƒê´€ê´€ê³„ ê¸°ëŒ€)

    def get_quality_grade(self, overall_quality: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if overall_quality >= 0.9:
            return "A+ (ìš°ìˆ˜)"
        elif overall_quality >= 0.8:
            return "A (ì–‘í˜¸)"
        elif overall_quality >= 0.7:
            return "B (ë³´í†µ)"
        elif overall_quality >= 0.6:
            return "C (ë¯¸í¡)"
        else:
            return "D (ë¶ˆëŸ‰)"

    def print_validation_report(self, results: Dict[str, float]):
        """ê²€ì¦ ë¦¬í¬íŠ¸ ì¶œë ¥"""

        print("\nğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
        print("=" * 50)
        print(f"ğŸ¯ ë°ì´í„° ë‹¤ì–‘ì„±:     {results['diversity_score']:.3f}")
        print(f"ğŸ® í˜„ì‹¤ì„±:          {results['realism_score']:.3f}")
        print(f"âš–ï¸ ë°¸ëŸ°ìŠ¤ ì»¤ë²„ë¦¬ì§€:   {results['balance_coverage']:.3f}")
        print(f"ğŸ”— ìƒê´€ê´€ê³„ ê°•ë„:     {results['correlation_strength']:.3f}")
        print("-" * 50)
        print(f"ğŸ† ì¢…í•© í’ˆì§ˆ:        {results['overall_quality']:.3f}")
        print(f"ğŸ“ í’ˆì§ˆ ë“±ê¸‰:        {results['quality_grade']}")

        # ê°œì„  ê¶Œì¥ì‚¬í•­
        if results['overall_quality'] < 0.8:
            print("\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
            if results['diversity_score'] < 0.8:
                print("  - ë” ë‹¤ì–‘í•œ íŒ€ êµ¬ì„±ê³¼ í™˜ê²½ ì¡°í•© í•„ìš”")
            if results['realism_score'] < 0.7:
                print("  - ì‹œë®¬ë ˆì´ì…˜ ë¡œì§ì˜ í˜„ì‹¤ì„± ê°œì„  í•„ìš”")
            if results['balance_coverage'] < 0.9:
                print("  - ê·¹ë‹¨ì  ë°¸ëŸ°ìŠ¤ ìƒí™© ë°ì´í„° ì¶”ê°€ í•„ìš”")
            if results['correlation_strength'] < 0.6:
                print("  - ì…ë ¥-ì¶œë ¥ ë…¼ë¦¬ì  ì—°ê´€ì„± ê°•í™” í•„ìš”")

class DataAugmentation:
    """ë°ì´í„° ì¦ê°• ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.augmentation_strategies = [
            'noise_injection',
            'parameter_scaling',
            'environment_variation',
            'skill_interpolation'
        ]

    def augment_dataset(self, training_data: List[Dict],
                       target_size: int = 50000) -> List[Dict]:
        """ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ë°ì´í„°ì…‹ í™•ì¥"""

        current_size = len(training_data)
        if current_size >= target_size:
            return training_data

        augmented_data = training_data.copy()
        needed_samples = target_size - current_size

        print(f"ğŸ”„ ë°ì´í„° ì¦ê°•: {current_size} â†’ {target_size} ìƒ˜í”Œ")

        for i in range(needed_samples):
            # ì›ë³¸ ìƒ˜í”Œ ëœë¤ ì„ íƒ
            base_sample = random.choice(training_data)

            # ì¦ê°• ì „ëµ ëœë¤ ì„ íƒ
            strategy = random.choice(self.augmentation_strategies)

            # ì¦ê°• ì ìš©
            augmented_sample = self.apply_augmentation(base_sample, strategy)
            augmented_data.append(augmented_sample)

            if (i + 1) % 5000 == 0:
                print(f"  ì§„í–‰ë¥ : {i+1}/{needed_samples}")

        print("âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ!")
        return augmented_data

    def apply_augmentation(self, sample: Dict, strategy: str) -> Dict:
        """ì¦ê°• ì „ëµ ì ìš©"""

        augmented_sample = copy.deepcopy(sample)

        if strategy == 'noise_injection':
            # ë§¤íŠ¸ë¦­ìŠ¤ì— ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€
            for matrix in augmented_sample['input_matrices']:
                noise = np.random.normal(0, 0.05, matrix.shape)
                matrix += noise
                matrix = np.clip(matrix, 0.1, 2.0)  # ë²”ìœ„ ì œí•œ

        elif strategy == 'parameter_scaling':
            # ì „ì²´ì ì¸ íŒŒì›Œ ìŠ¤ì¼€ì¼ë§
            scale_factor = random.uniform(0.9, 1.1)
            for matrix in augmented_sample['input_matrices']:
                matrix *= scale_factor

        elif strategy == 'environment_variation':
            # í™˜ê²½ ì¡°ê±´ ë³€ê²½
            environments = {
                'time': ['day', 'night', 'dawn', 'dusk'],
                'weather': ['clear', 'rain', 'storm', 'snow'],
                'terrain': ['plain', 'forest', 'mountain', 'desert']
            }

            env = augmented_sample['game_state']['environment']
            for key, options in environments.items():
                if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ë³€ê²½
                    env[key] = random.choice(options)

        elif strategy == 'skill_interpolation':
            # í”Œë ˆì´ì–´ ìŠ¤í‚¬ ë ˆë²¨ ë³´ê°„
            current_skill = augmented_sample['game_state']['avg_skill_level']
            skill_variation = random.uniform(-0.1, 0.1)
            new_skill = np.clip(current_skill + skill_variation, 0.0, 1.0)
            augmented_sample['game_state']['avg_skill_level'] = new_skill

        return augmented_sample

## ğŸ¤– ì˜¤í† ì¸ì½”ë” í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ

class IntegratedTrainingPipeline:
    """ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° + ì˜¤í† ì¸ì½”ë” í†µí•© í•™ìŠµ íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        self.simulator = GameOutcomeSimulator()
        self.validator = SimulationDataValidator()
        self.augmenter = DataAugmentation()
        self.autoencoder = None

    def run_complete_pipeline(self,
                            initial_samples: int = 10000,
                            target_samples: int = 50000,
                            validation_split: float = 0.2) -> Dict:
        """ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        print("ğŸš€ Defense Allies í†µí•© í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)

        # 1. ì´ˆê¸° ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        print("\n1ï¸âƒ£ ì´ˆê¸° ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±")
        raw_data = generate_simulation_training_data(initial_samples)

        # 2. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        print("\n2ï¸âƒ£ ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
        quality_results = self.validator.validate_dataset(raw_data)

        # 3. í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ë°ì´í„° ê°œì„ 
        if quality_results['overall_quality'] < 0.7:
            print("\nâš ï¸ ë°ì´í„° í’ˆì§ˆ ê°œì„  í•„ìš” - ì¶”ê°€ ìƒì„± ì¤‘...")
            additional_data = generate_simulation_training_data(initial_samples // 2)
            raw_data.extend(additional_data)
            quality_results = self.validator.validate_dataset(raw_data)

        # 4. ë°ì´í„° ì¦ê°•
        print("\n3ï¸âƒ£ ë°ì´í„° ì¦ê°•")
        augmented_data = self.augmenter.augment_dataset(raw_data, target_samples)

        # 5. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
        print("\n4ï¸âƒ£ ë°ì´í„° ë¶„í• ")
        train_data, val_data = self.split_dataset(augmented_data, validation_split)

        # 6. ì˜¤í† ì¸ì½”ë” í•™ìŠµ
        print("\n5ï¸âƒ£ ì˜¤í† ì¸ì½”ë” í•™ìŠµ")
        training_results = self.train_autoencoder(train_data, val_data)

        # 7. ìµœì¢… ì„±ëŠ¥ í‰ê°€
        print("\n6ï¸âƒ£ ìµœì¢… ì„±ëŠ¥ í‰ê°€")
        final_performance = self.evaluate_final_performance(val_data)

        # 8. ê²°ê³¼ ìš”ì•½
        pipeline_results = {
            'data_quality': quality_results,
            'training_results': training_results,
            'final_performance': final_performance,
            'dataset_size': len(augmented_data),
            'pipeline_success': final_performance['overall_score'] > 0.8
        }

        self.print_pipeline_summary(pipeline_results)
        return pipeline_results

    def split_dataset(self, data: List[Dict], validation_split: float) -> Tuple[List[Dict], List[Dict]]:
        """ë°ì´í„°ì…‹ ë¶„í• """

        random.shuffle(data)
        split_idx = int(len(data) * (1 - validation_split))

        train_data = data[:split_idx]
        val_data = data[split_idx:]

        print(f"  í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
        print(f"  ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")

        return train_data, val_data

    def train_autoencoder(self, train_data: List[Dict], val_data: List[Dict]) -> Dict:
        """ì˜¤í† ì¸ì½”ë” í•™ìŠµ"""

        # PyTorch ë°ì´í„°ì…‹ ë³€í™˜
        train_dataset = self.convert_to_pytorch_dataset(train_data)
        val_dataset = self.convert_to_pytorch_dataset(val_data)

        # ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ì´ˆê¸°í™”
        from autoencoder_balancing_system import BalanceAutoencoder, BalanceTrainer

        self.autoencoder = BalanceAutoencoder()
        trainer = BalanceTrainer(self.autoencoder)

        # í•™ìŠµ ì‹¤í–‰
        trainer.train(num_epochs=100, batch_size=64)

        # ê²€ì¦ ì„±ëŠ¥ ì¸¡ì •
        val_loss = trainer.evaluate(val_dataset)

        # ëª¨ë¸ ì €ì¥
        trainer.save_model('defense_allies_trained_autoencoder.pth')

        return {
            'final_train_loss': trainer.training_history[-1]['total_loss'],
            'validation_loss': val_loss,
            'training_epochs': len(trainer.training_history),
            'convergence_achieved': val_loss < 0.1
        }

    def convert_to_pytorch_dataset(self, data: List[Dict]):
        """ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ PyTorch ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜"""

        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë³€í™˜ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°œë…ì  êµ¬ì¡°ë§Œ ì œì‹œ

        input_matrices = []
        game_states = []
        target_outcomes = []

        for sample in data:
            # ë§¤íŠ¸ë¦­ìŠ¤ í‰íƒ„í™”
            matrices_flat = np.concatenate([
                matrix.flatten() for matrix in sample['input_matrices']
            ])
            input_matrices.append(matrices_flat)

            # ê²Œì„ ìƒíƒœ ì¸ì½”ë”©
            game_state_encoded = self.encode_game_state_vector(sample['game_state'])
            game_states.append(game_state_encoded)

            # ëª©í‘œ ê²°ê³¼ ì¸ì½”ë”©
            outcome_encoded = self.encode_target_outcome(sample['target_outcome'])
            target_outcomes.append(outcome_encoded)

        # PyTorch í…ì„œë¡œ ë³€í™˜
        import torch
        from torch.utils.data import TensorDataset

        dataset = TensorDataset(
            torch.FloatTensor(input_matrices),
            torch.FloatTensor(game_states),
            torch.FloatTensor(target_outcomes)
        )

        return dataset

    def encode_game_state_vector(self, game_state: Dict) -> np.ndarray:
        """ê²Œì„ ìƒíƒœë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©"""

        # 10ì°¨ì› ë²¡í„°ë¡œ ì¸ì½”ë”©
        vector = np.zeros(10)

        vector[0] = len(game_state['team_composition']) / 4.0  # íŒ€ í¬ê¸°
        vector[1] = game_state['avg_skill_level']  # í‰ê·  ìŠ¤í‚¬

        # í™˜ê²½ ì›-í•« ì¸ì½”ë”© (ê°„ë‹¨í™”)
        time_encoding = {'day': 0, 'night': 1, 'dawn': 2, 'dusk': 3}
        vector[2] = time_encoding.get(game_state['environment']['time'], 0) / 3.0

        # ë‚˜ë¨¸ì§€ ì°¨ì›ë“¤ë„ ìœ ì‚¬í•˜ê²Œ ì¸ì½”ë”©...

        return vector

    def encode_target_outcome(self, outcome: Dict) -> np.ndarray:
        """ëª©í‘œ ê²°ê³¼ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©"""

        # 3ì°¨ì› ì ì¬ ë²¡í„°ë¡œ ì¸ì½”ë”© (ì˜¤í† ì¸ì½”ë” ë³´í‹€ë„¥ê³¼ ì¼ì¹˜)
        latent_vector = np.zeros(3)

        # ìŠ¹ë¥  â†’ ë‚œì´ë„ ì¡°ì •
        win_prob = outcome['win_probability']
        if win_prob > 0.6:
            latent_vector[0] = (win_prob - 0.6) / 0.4  # ì‰½ê²Œ ì¡°ì •
        elif win_prob < 0.4:
            latent_vector[0] = -(0.4 - win_prob) / 0.4  # ì–´ë µê²Œ ì¡°ì •

        # ë°¸ëŸ°ìŠ¤ í’ˆì§ˆ â†’ ë°¸ëŸ°ìŠ¤ ëª©í‘œ
        latent_vector[1] = outcome['balance_quality']

        # í˜‘ë ¥ ìˆ˜ì¤€ â†’ í˜‘ë ¥ ê°€ì¤‘ì¹˜
        latent_vector[2] = outcome['cooperation_level']

        return latent_vector

    def evaluate_final_performance(self, val_data: List[Dict]) -> Dict:
        """ìµœì¢… ì„±ëŠ¥ í‰ê°€"""

        # ì‹¤ì œ ê²Œì„ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í…ŒìŠ¤íŠ¸
        test_scenarios = self.generate_test_scenarios()

        performance_scores = []

        for scenario in test_scenarios:
            # ì˜¤í† ì¸ì½”ë” ì˜ˆì¸¡
            predicted_result = self.autoencoder.predict(scenario)

            # ì‹œë®¬ë ˆì´ì…˜ ì‹¤ì œ ê²°ê³¼
            actual_result = self.simulator.simulate_full_game(scenario)

            # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
            accuracy = self.calculate_prediction_accuracy(predicted_result, actual_result)
            performance_scores.append(accuracy)

        overall_score = np.mean(performance_scores)

        return {
            'prediction_accuracy': overall_score,
            'test_scenarios': len(test_scenarios),
            'performance_grade': 'A' if overall_score > 0.9 else 'B' if overall_score > 0.8 else 'C',
            'overall_score': overall_score
        }

    def print_pipeline_summary(self, results: Dict):
        """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""

        print("\n" + "="*60)
        print("ğŸ† Defense Allies í†µí•© í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print("="*60)

        print(f"\nğŸ“Š ë°ì´í„° í’ˆì§ˆ: {results['data_quality']['quality_grade']}")
        print(f"ğŸ¯ ë°ì´í„°ì…‹ í¬ê¸°: {results['dataset_size']:,}ê°œ")
        print(f"ğŸ¤– í•™ìŠµ ìˆ˜ë ´: {'ì„±ê³µ' if results['training_results']['convergence_achieved'] else 'ì‹¤íŒ¨'}")
        print(f"ğŸ“ˆ ìµœì¢… ì„±ëŠ¥: {results['final_performance']['performance_grade']}")
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ: {'ì˜ˆ' if results['pipeline_success'] else 'ì•„ë‹ˆì˜¤'}")

        if results['pipeline_success']:
            print("\nğŸ‰ Defense Allies ì˜¤í† ì¸ì½”ë” ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("   ì´ì œ ì‹¤ì‹œê°„ ê²Œì„ ë°¸ëŸ°ì‹±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            print("\nâš ï¸ ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   ë°ì´í„° í’ˆì§ˆ ë˜ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ê²€í† í•˜ì„¸ìš”.")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    pipeline = IntegratedTrainingPipeline()
    results = pipeline.run_complete_pipeline(
        initial_samples=5000,
        target_samples=25000,
        validation_split=0.2
    )
```

## ğŸ¯ ì‹œë®¬ë ˆì´ì…˜ ì‹œìŠ¤í…œì˜ í˜ì‹ ì  ê°€ì¹˜

### 1. ì‹¤ì œ í”Œë ˆì´ ì—†ì´ í•™ìŠµ ë°ì´í„° í™•ë³´
- **10,000+ ì‹œë®¬ë ˆì´ì…˜**: ë‹¤ì–‘í•œ ê²Œì„ ìƒí™© ì™„ì „ ì»¤ë²„
- **í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ**: A+ ë“±ê¸‰ ë°ì´í„° ë³´ì¥
- **ë°ì´í„° ì¦ê°•**: 50,000+ ìƒ˜í”Œë¡œ í™•ì¥

### 2. ê³¼í•™ì  ê²Œì„ ë©”ì»¤ë‹ˆì¦˜ ëª¨ë¸ë§
- **ì „íˆ¬ ì‹œë®¬ë ˆì´ì…˜**: íƒ€ì›Œ vs ì  ìˆ˜í•™ì  ê³„ì‚°
- **ì‹œë„ˆì§€ ì‹œë®¬ë ˆì´ì…˜**: 18Ã—18 ì¢…ì¡± ìƒí˜¸ì‘ìš©
- **í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜**: 120ê°€ì§€ í™˜ê²½ ì¡°í•© íš¨ê³¼

### 3. í”Œë ˆì´ì–´ í–‰ë™ íŒ¨í„´ ëª¨ë¸ë§
- **4ê°€ì§€ ìŠ¤í‚¬ ë ˆë²¨**: ì´ˆë³´ì â†’ ì „ë¬¸ê°€
- **4ê°€ì§€ í˜‘ë ¥ ìŠ¤íƒ€ì¼**: ì†”ë¡œ â†’ ê²½ìŸì 
- **ì˜ì‚¬ê²°ì • ì‹œë®¬ë ˆì´ì…˜**: í˜„ì‹¤ì  í”Œë ˆì´ì–´ í–‰ë™

### 4. ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸
- **ë°ì´í„° ìƒì„± â†’ ê²€ì¦ â†’ ì¦ê°• â†’ í•™ìŠµ â†’ í‰ê°€**
- **í’ˆì§ˆ ë³´ì¥**: ìë™ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 
- **ì„±ëŠ¥ ì¸¡ì •**: A/B/C ë“±ê¸‰ ìë™ í‰ê°€

**Defense AlliesëŠ” ì´ì œ ì‹¤ì œ í”Œë ˆì´ì–´ ì—†ì´ë„ ì™„ë²½í•œ AI ë°¸ëŸ°ì‹± ì‹œìŠ¤í…œì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!** ğŸ¤–

---

**ë‹¤ìŒ ë‹¨ê³„**: ì‹¤ì œ ê²Œì„ ì„œë²„ ë°°í¬ ë° ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì¶•
